{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "# Measuring Classifier Performance\n",
    "What are our learning objectives for this lesson?\n",
    "* Measure and evaluate classifier performance using different metrics\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes\n",
    "\n",
    "## Today 10/24\n",
    "* Announcements\n",
    "    * RAG Workshop Details:  \n",
    "      <img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/workshop.png\" width=\"300\">  \n",
    "    * LA8 is canceled.\n",
    "    * PA4 is due today.\n",
    "    * PA5 will be posted later today.\n",
    "        * See [ClassificationFun/main.py](https://github.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/blob/main/ClassificationFun/main.py) for a useful function you may want to use for PA5: `randomize_in_place(alist, parallel_list=None)`. This function accepts at least one list and shuffles its elements. If a second list is provided, it is shuffled in the same order.\n",
    "    * Paper Review Presentation on (Oct 27 and Oct 29). Please check the annoucement on Canvas to confirm your assigned day and turn.\n",
    "* Handout for Paper Review Questions/Comments will be distributed today.\n",
    "* Discussion on paper review. Any question? \n",
    "* Finish Naive Bayes Lab Task #2\n",
    "* Measuring Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Accuracy: Additional Performance Evaluation Metrics\n",
    "So far, we have looked at accuracy, which tells us the proportion of correctly classified instances.  \n",
    "\n",
    "However, accuracy doesn’t always tell the whole story, especially when the classes are imbalanced.  \n",
    "\n",
    "#### Example: Cancer Dataset\n",
    "Suppose we want to classify tumors as *malignant* or *benign*.  \n",
    "Imagine the dataset has 95% benign and 5% malignant samples.\n",
    "\n",
    "|                | Predicted Malignant | Predicted Benign |\n",
    "|--|--|--|\n",
    "| Actual Malignant    | 0| 50|\n",
    "| Actual Benign | 0| 950|\n",
    "\n",
    "If our classifier predicts everything as benign, it achieves **95% accuracy**, but fails to detect any malignant tumors! This is clearly misleading in a medical context.\n",
    "\n",
    "#### Why We Need Other Metrics\n",
    "To better evaluate a classifier, especially for the minority class, we use other metrics. In Bramer Chapter 12, there is a nice table summarizing commonly used performance metrics for a classifier:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/bramer_perf_measures.png\" width=\"600\">\n",
    "\n",
    "### Error Rate\n",
    "Error Rate: 1 - accuracy\n",
    "$$ErrorRate = \\frac{FP + FN}{P + N}$$\n",
    "* Has same issues as accuracy (unbalanced labels)\n",
    "* For multi-class classification, can take the average error rate per class\n",
    "\n",
    "### Precision\n",
    "Precision (positive predictive value): Proportion of instances classified as positive that are really positive\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "* A measure of \"exactness\"\n",
    "* When a classifier predicts positive, it is correct $precision$ percent of the time\n",
    "* A classifier with no false positives has a precision of 1\n",
    "* In high-stakes scenarios like the death penalty, false positives are critical, high precision ensures we minimize these errors.\n",
    "\n",
    "\n",
    "### Recall\n",
    "Recall (true positive rate (TPR) AKA sensitivity): The proportion of positive instances that are correctly classified as positive (e.g. labeled correctly)\n",
    "$$Recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}$$\n",
    "* A measure of \"completeness\"\n",
    "* A classifier correctly classifies $recall$ percent of all positive cases\n",
    "* A classifier with no false negatives has a recall of 1\n",
    "* Used with the false positive rate to create receiver operator graphs and curves (ROC)\n",
    "* In medical applications like cancer detection, false negatives are life-threatening, high recall ensures we capture as many actual positives as possible.\n",
    "\n",
    "Note: There is a trade-off between precision and recall. For a balanced class dataset, a model that predicts mostly positive examples will have a high recall and a low precision.\n",
    "\n",
    "Q: How can we get a high recall score?\n",
    "* Label everything as positive. This ensures you capture all actual positives (high recall).\n",
    "\n",
    "Q: What about for precision?\n",
    "* Be conservative with positive labels. Only label as positive when very confident.\n",
    "\n",
    "### F1 Score \n",
    "F1-Score (F-Measure): combines precision and recall via the harmonic mean of precision and recall:\n",
    "$$F = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n",
    "* Summarizes a classifier in a single number (however, it is best practice to still investigate precision and recall, as well as other evaluation metrics)\n",
    "* Alternatively, we can weight precision using the F- Beta score:\n",
    "$$F_\\beta = \\frac{(1+\\beta^2) \\times Precision \\times Recall}{\\beta^2 \\times Precision + Recall}$$\n",
    "- Here, $\\beta$ determines the relative importance of recall vs precision:\n",
    "   - $\\beta$ > 1 → recall is more important than precision\n",
    "   - $\\beta$ < 1 → precision is more important than recall\n",
    "   - $\\beta$ = 1 → precision and recall are equally important (this is the standard F1-score)\n",
    "- This is useful when classes are imbalanced or when one type of error is more costly than the other. For example:\n",
    "   - In medical diagnosis, false negatives are dangerous: use $\\beta$ > 1 to emphasize recall.\n",
    "   - In spam detection, false positives are costly: use $\\beta$ < 1 to emphasize precision.\n",
    "\n",
    "\n",
    "Note: Sci-kit Learn's [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) returns multi-class precision, recall, f1-score, and support given parallel lists of actual and predicted values.\n",
    "\n",
    "### Lab Task 1\n",
    "What is the precision, recall, and F-measure for the win-lose (binary) example?  \n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/accuracy_exercise.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        lose       0.80      0.40      0.53        20\n",
      "         win       0.60      0.90      0.72        20\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.70      0.65      0.63        40\n",
      "weighted avg       0.70      0.65      0.63        40\n",
      "\n",
      "{'lose': {'precision': 0.8, 'recall': 0.4, 'f1-score': 0.5333333333333333, 'support': 20.0}, 'win': {'precision': 0.6, 'recall': 0.9, 'f1-score': 0.72, 'support': 20.0}, 'accuracy': 0.65, 'macro avg': {'precision': 0.7, 'recall': 0.65, 'f1-score': 0.6266666666666667, 'support': 40.0}, 'weighted avg': {'precision': 0.7, 'recall': 0.65, 'f1-score': 0.6266666666666666, 'support': 40.0}}\n"
     ]
    }
   ],
   "source": [
    "# check trace desk calculation with sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# build parallel lists to represent win lose predictions\n",
    "y_true = [\"win\"] * 20 + [\"lose\"] * 20\n",
    "y_pred = [\"win\"] * 18 + [\"lose\"] * 2\n",
    "y_pred += [\"win\"] * 12 + [\"lose\"] * 8\n",
    "\n",
    "# note that \"support\" is P, is the number of instances in the test set with the positive label\n",
    "# classification_report() reports metrics once treating \"win\" as the positive class\n",
    "# and once treating \"lose\" as the positive class \n",
    "# nice text/table form\n",
    "print(classification_report(y_true, y_pred))\n",
    "# returned dictionary form\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "print(report_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F-Measure for Multi-class Classification\n",
    "\"Micro\" average $\\mu$\n",
    "* Averaging the total true positives, false negatives and false positives\n",
    "    * E.g. compute TP and FP (or FN) over all the labels to compute precision (or recall))\n",
    "* Micro-averaging favors bigger classes\n",
    "\n",
    "$$Precision_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L} (TP_i + FP_i)}$$\n",
    "\n",
    "$$Recall_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L}(TP_i + FN_i)}$$\n",
    "\n",
    "$$F_\\mu = \\frac{2 \\times Precision_\\mu \\times Recall_\\mu}{Precision_\\mu + Recall_\\mu}$$\n",
    "\n",
    "\"Macro\" averaging $M$\n",
    "* Averaging the unweighted mean per label\n",
    "    * E.g. compute each label's precision (or recall) and average over number of labels\n",
    "* Macro-averaging treats all classes equally\n",
    "$$Precision_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FP_i}}{L}$$\n",
    "\n",
    "$$Recall_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FN_i}}{L}$$\n",
    "\n",
    "$$F_M = \\frac{\\sum_{i=1}^{L} \\frac{2 * Precision_{Mi} * Recall_{Mi}}{Precision_{Mi} + Recall_{Mi}}}{L}$$\n",
    "\n",
    "\"Weighted\" macro averaging $W$\n",
    "* Averaging the support-weighted mean per label\n",
    "    * E.g. like macro average, but compute each label's precision (or recall) then weight it by its count $P$ (AKA support) and average over the total number of instances\n",
    "$$Precision_W = \\frac{\\sum_{i=1}^{L}P_i \\times \\frac{TP_i}{TP_i + FP_i}}{P + N}$$\n",
    "\n",
    "$$Recall_W = \\frac{\\sum_{i=1}^{L}P_i \\times \\frac{TP_i}{TP_i + FN_i}}{P + N}$$\n",
    "\n",
    "$$F_W = \\frac{\\sum_{i=1}^{L} P_i \\times \\frac{2 * Precision_{Wi} * Recall_{Wi}}{Precision_{Wi} + Recall_{Wi}}}{P + N}$$\n",
    "\n",
    "### Lab Task 2\n",
    "What is the precision, recall, and F-measure for the coffee acidity (multi-class) example?\n",
    "1. Using the \"Micro\" average approach\n",
    "1. Using the \"Macro\" average approach\n",
    "1. Using the \"Weighted\" macro average approach\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/multi_class_accuracy_exercise.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dry      0.800     0.800     0.800        25\n",
      "        dull      0.480     0.400     0.436        30\n",
      "    moderate      0.720     0.600     0.655        30\n",
      "       sharp      0.500     0.750     0.600        20\n",
      "\n",
      "    accuracy                          0.619       105\n",
      "   macro avg      0.625     0.638     0.623       105\n",
      "weighted avg      0.629     0.619     0.616       105\n",
      "\n",
      "{'dry': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 25}, 'dull': {'precision': 0.48, 'recall': 0.4, 'f1-score': 0.4363636363636364, 'support': 30}, 'moderate': {'precision': 0.72, 'recall': 0.6, 'f1-score': 0.6545454545454547, 'support': 30}, 'sharp': {'precision': 0.5, 'recall': 0.75, 'f1-score': 0.6, 'support': 20}, 'accuracy': 0.6190476190476191, 'macro avg': {'precision': 0.625, 'recall': 0.6375000000000001, 'f1-score': 0.6227272727272728, 'support': 105}, 'weighted avg': {'precision': 0.6285714285714286, 'recall': 0.6190476190476191, 'f1-score': 0.6164502164502165, 'support': 105}}\n"
     ]
    }
   ],
   "source": [
    "# check trace desk calculation with sklearn\n",
    "# build parallel lists to represent coffee acidity predictions\n",
    "y_true = [\"dry\"] * 25 + [\"sharp\"] * 20 + [\"moderate\"] * 30 + [\"dull\"] * 30\n",
    "y_pred = [\"dry\"] * 20 + [\"sharp\"] * 2 + [\"moderate\"] * 2 + [\"dull\"] * 1\n",
    "y_pred += [\"dry\"] * 0 + [\"sharp\"] * 15 + [\"moderate\"] * 1 + [\"dull\"] * 4\n",
    "y_pred += [\"dry\"] * 1 + [\"sharp\"] * 3 + [\"moderate\"] * 18 + [\"dull\"] * 8\n",
    "y_pred += [\"dry\"] * 4 + [\"sharp\"] * 10 + [\"moderate\"] * 4 + [\"dull\"] * 12\n",
    "\n",
    "# nice text/table form\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "# returned dictionary form\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "print(report_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate \n",
    "False Positive Rate (FPR): The proportion of negative instances that are erroneously classified as positive\n",
    "$$False Positive Rate = \\frac{FP}{N} = \\frac{FP}{TN + FP}$$\n",
    "* Used with the true positive rate to create receiver operator graphs and curves (ROC)\n",
    "\n",
    "### False Negative Rate \n",
    "False Negative Rate (FNR): The proportion of positive instances that are erroneously classified as negative = 1 − True Positive Rate\n",
    "$$False Negative Rate = \\frac{FN}{P} = \\frac{FN}{TP + FN}$$\n",
    "\n",
    "### True Negative Rate \n",
    "True Negative Rate (TNR AKA specificity): The proportion of negative instances that are correctly classified as negative\n",
    "$$False Negative Rate = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
