{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Estimating Classifier Accuracy\n",
    "What are our learning objectives for this lesson?\n",
    "* Divide a dataset into training and testing sets using different approaches\n",
    "* Evaluate classifier performance using accuracy\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:  \n",
    "* Dr. Gina Sprint's Data Science Algorithm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Midterm this Friday (17th oct) (covers all topics up to KNN. Review notebooks and IQs)\n",
    "* Estimating classifier accuracy\n",
    "    * Dividing a dataset into training and test sets\n",
    "    * Lab tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Building a classifier starts with a learning (training) phase\n",
    "* Based on predefined set of examples (training set)\n",
    "\n",
    "The classifier is then evaluated for predictive accuracy (% of test instances correctly classified by the classifier)\n",
    "* Based on another set of examples (testing set)\n",
    "* We use the actual labels of the examples to test the predictions\n",
    "\n",
    "In general, we want to try to avoid overfitting  \n",
    "* This occurs when the model learns specific details or noise from the training data, capturing patterns that don’t generalize to new data.\n",
    "* A related issue is underfitting, which happens when the model is too simple to capture the true relationships in the data (e.g., using a linear model for nonlinear patterns).\n",
    "\n",
    "We are going to discuss different ways to select training and testing sets\n",
    "1. The Holdout method\n",
    "2. Random Subsampling\n",
    "3. $k$-Fold Cross Validation and Variants\n",
    "4. Bootstrap Method\n",
    "\n",
    "### Holdout Method\n",
    "In the holdout method, the dataset is divided into two sets, the training and the testing set. The training set is used to build the model and the testing set is used to evaluate the model (e.g. the model's accuracy).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "\n",
    "Approaches to the holdout method\n",
    "* Randomly divide data set into a training and test set\n",
    "* Partition the data evenly or use a predefined ratio, e.g., $\\frac{2}{3}$ for training and $\\frac{1}{3}$ for testing (2:1 ratio)\n",
    "* The division is done through random selection without replacement, meaning each data point appears in only one of the two sets.\n",
    "\n",
    "\n",
    "Q: Write a function to do a 2:1 partition in Python...\n",
    "```python\n",
    "import random\n",
    "\n",
    "def compute_holdout_partitions(table):\n",
    "    # randomize the table\n",
    "    randomized = table[:] # copy the table\n",
    "    n = len(table)\n",
    "    for i in range(n):\n",
    "        # pick an index to swap\n",
    "        j = random.randrange(0, n) # random int in [0,n) \n",
    "        randomized[i], randomized[j] = randomized[j], randomized[i]\n",
    "    # return train and test sets\n",
    "    split_index = int(2 / 3 * n) # 2/3 of randomized table is train, 1/3 is test\n",
    "    return randomized[0:split_index], randomized[split_index:]\n",
    "```\n",
    "\n",
    "### Random Subsampling Method\n",
    "* Repeat the holdout method $k$ times\n",
    "* Accuracy estimate is the average of the accuracy of each iteration\n",
    "\n",
    "### k-Fold Cross-Validation Method\n",
    "One of the shortcomings of the hold out method is the evaluation of the model depends heavily on which examples are selected for training versus testing. K-fold cross validation is a model evaluation approach that addresses this shortcoming of the holdout method.\n",
    "* Initial dataset partitioned into $k$ subsets (\"folds\") $D_1, D_2,..., D_k$\n",
    "* Each fold is approximately the same size\n",
    "* Training and testing is performed $k$ times:\n",
    "    * In iteration $i$, $D_i$ is used as the test set\n",
    "    * And $D_1 \\cup ... \\cup D_{i−1} \\cup D_{i+1} \\cup ... \\cup D_k$ used as training set\n",
    "* Note each subset is used exactly once for testing\n",
    "* Accuracy estimate is number of correct classifications over the $k$ iterations, divided by total number of rows (i.e., test instances) in the initial dataset\n",
    "* Alternatively, average accuracy by label\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "\n",
    "(image from [wikimedia](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg))\n",
    "\n",
    "### Variants of Cross-Validation\n",
    "Leave-one-out method\n",
    "* Special case of cross-validation where $k$ is the number of instances\n",
    "\n",
    "Stratified Cross-Validation method\n",
    "* Class distribution within folds is approximately the same as in the initial data\n",
    "\n",
    "Q: How might you go about generating stratified folds for cross validation?\n",
    "* One approach:\n",
    "    * Randomize the dataset\n",
    "    * Partition dataset so each subset contains rows with of a specific class\n",
    "        * e.g., if class label is \"yes\" or \"no\"\n",
    "        * Then one partition has all \"yes\" rows\n",
    "        * And the other all \"no\" rows\n",
    "        * Note: this is a group by class label\n",
    "    * Generate folds by:\n",
    "        * Iterating through each partition\n",
    "        * And distributing the partition (roughly) equally to each fold\n",
    "        \n",
    "### Lab Task 1\n",
    "Consider the following dataset:\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Assume we want to perform k-fold cross validation of our NN classifier for $k$ = 4. Create corresponding folds (partitions) for the dataset.\n",
    "2. Describe how these $k$ folds would be used to perform cross validation. That is, show how the $k$ test runs are performed.\n",
    "3. Repeat steps 1 and 2 with *stratified* k-fold cross validation.\n",
    "\n",
    "    \n",
    "### The Bootstrap Method\n",
    "* Like random subsampling but with replacement\n",
    "* Usually used for small datasets\n",
    "* The basic \".632\" approach:\n",
    "    * Given a dataset with $D$ rows\n",
    "    * Randomly select $D$ rows with replacement (i.e., might select same row)\n",
    "    * This gives a \"bootstrap sample\" (training set) of $D$ rows\n",
    "    * The remaining rows (not selected; AKA out of bag instances) form the test set\n",
    "    * On average, 63.2% of original rows will end up in the training set\n",
    "    * And 36.8% will end up in the test set\n",
    "* Why these percentages?\n",
    "    * Each row has a $1/D$ chance of being selected\n",
    "    * Each row has a (1 - 1/D) chance of not being selected\n",
    "    * We select $D$ times, so probability a row not chosen at all is $(1 - 1/D)^D$\n",
    "    * For large $D$, the probability approaches $e^{-1} = 0.368$ (for $e = 2.718$...)\n",
    "* The sampling procedure is repeated $k$ times\n",
    "    * Each iteration uses the test set for an accuracy estimate\n",
    "    * Since each test can be a different size based on the sampling, the accuracy is the weighted average accuracy over the $k$ bootstrap methods\n",
    "        * See Bramer 7.2.2 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
