{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Nearest Neighbor Classification\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the kNN classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/8\n",
    "* Announcements\n",
    "    * IQ4 on Friday (10th Oct)\n",
    "    * Paper review description posted on Canvas (please select the paper and put your name on the excel sheet)\n",
    "    * PA3 is due today\n",
    "    * PA4 will be assigned tomorrow\n",
    "* kNN overview and algorithm trace\n",
    "\n",
    "## Today 10/10\n",
    "* Annoucement\n",
    "    * Work on the KNN examples\n",
    "    * Discuss PA4\n",
    "* IQ4 (last 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification\n",
    "Nearest Neighbor Classification is one of the simplest forms of machine learning, often used when attributes are continuous, though it can be adapted for categorical features as well.\n",
    "\n",
    "The core intuition is:\n",
    "**An instance is likely to have the same label as its closest neighbor.**\n",
    "\n",
    "**Basic approach:**\n",
    "* Given an instance $i$ with $n - 1$ attributes (where the $n^{th}$ is class label)\n",
    "* Find the \"closest\" instance $j$ to $i$ on the $n - 1$ attributes\n",
    "* Use $j$'s class as the prediction for $i$\n",
    "\n",
    "Example from book:\n",
    "Given the data set\n",
    "\n",
    "|a |b |c |d |e |f |Class|\n",
    "|-|-|-|-|-|-|-|\n",
    "|yes |no |no |6.4 |8.3 |low |negative|\n",
    "|yes |yes |yes |18.2 |4.7 |high |positive|\n",
    "\n",
    "What should this instance's classification be?\n",
    "\n",
    "|yes |no |no |6.6 |8.0 |low |???|\n",
    "|-|-|-|-|-|-|-|\n",
    "\n",
    "Usually it isn't this easy!\n",
    "\n",
    "---\n",
    "\n",
    "## k Nearest Neighbors\n",
    "While nearest neighbor (k = 1) works in simple scenarios, it’s very sensitive to noise, one mislabeled point can lead to incorrect predictions.\n",
    "\n",
    "That’s where k-Nearest Neighbors comes in:\n",
    "In the k-Nearest Neighbors algorithm, k is just the number of nearby neighbors your data point peeks at before making a decision. \n",
    "It checks out the k closest neighbors and joins the group that most of them belong to.\n",
    "Think of it as moving into a neighborhood based on who lives nearby!  \n",
    "K-Nearest Neighbors is also called as a lazy learning algorithm because it does not learn from the training set immediately instead it stores the entire dataset and performs computations only at the time of classification.\n",
    "\n",
    "\n",
    "Instead of using just one closest neighbor, we:\n",
    "\n",
    "* Find the k nearest neighbors,\n",
    "* Take a vote among them,\n",
    "* Assign the class with the majority vote.\n",
    "![KNN Example](https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/knn.png)\n",
    "\n",
    "*Image source: [GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20200616145419/Untitled2781.png)*\n",
    "\n",
    "Think of **KNN** like the new kid in town trying to figure out which group to hang out with. \n",
    "\n",
    "In the picture, we have got two squads:  \n",
    "- Red diamonds = Category 1\n",
    "- Blue squares = Category 2  \n",
    "\n",
    "\n",
    "A new data point shows up (let’s call it “Dotty”). Dotty looks around and checks out the closest crew (the circled neighbors). Most of them are blue squares, so Dotty decides, *“Hey, I guess I’m one of them!”* \n",
    "\n",
    "That’s KNN in action, it predicts based on:\n",
    "- Who you're closest to (proximity)\n",
    "- Which group has the majority (majority vote)\n",
    "\n",
    "**In short:**  \n",
    "> proximity + popularity = prediction!\n",
    "\n",
    "\n",
    "\n",
    "### Choosing the Best Value for k in k-NN\n",
    "\n",
    "The value of *k* (number of neighbors) plays a crucial role in k-NN performance. There is no one-size-fits-all rule, but a common approach to finding a good *k* includes:\n",
    "\n",
    "1. Split your dataset into training and test sets.\n",
    "2. Try different values of *k* (starting from 1 upwards).\n",
    "3. For each *k*, train the model and compute its error rate on the test set.\n",
    "4. Select the *k* that yields the lowest test error.\n",
    "\n",
    "Notes:\n",
    "- Odd values of *k* are typically preferred for binary classification to avoid ties.\n",
    "- As a rule of thumb, the larger your training data, the larger the *k* you can use effectively.\n",
    "\n",
    "\n",
    "Lets say we found the k nearest neighbors for an instance ...\n",
    "\n",
    "Q: What are ways we could pick the class?\n",
    "* Most frequent occurring class\n",
    "* Weighted \"average\" (based on the relative closest of the k)\n",
    "\n",
    "Note: can use k-NN for regression if, e.g., return the mean of the label values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does k-NN Know Who’s Nearby?\n",
    "\n",
    "The **k-Nearest Neighbors (k-NN)** algorithm works by measuring distances between data points, basically figuring out who's close to whom in the neighborhood!\n",
    "\n",
    "### Distance Measures: The Heart of k-NN\n",
    "\n",
    "There are many ways to measure how \"far\" two points are from each other. These are called **distance metrics**.\n",
    "\n",
    "For two points, say `x` and `y`, the distance between them is:\n",
    "\n",
    "$$\n",
    "\\text{dist}(x, y)\n",
    "$$\n",
    "\n",
    "To be a valid **distance metric**, this function must satisfy:\n",
    "\n",
    "1. **Identity**  \n",
    "   $$\n",
    "   \\text{dist}(x, x) = 0\n",
    "   $$  \n",
    "   A point is zero distance from itself.\n",
    "\n",
    "2. **Symmetry**  \n",
    "   $$\n",
    "   \\text{dist}(x, y) = \\text{dist}(y, x)\n",
    "   $$  \n",
    "   Distance works the same in both directions.\n",
    "\n",
    "3. **Triangle Inequality**  \n",
    "   $$\n",
    "   \\text{dist}(x, y) \\leq \\text{dist}(x, z) + \\text{dist}(z, y)\n",
    "   $$  \n",
    "   The shortest path between two points is a straight line!\n",
    "\n",
    "---\n",
    "\n",
    "## Most Common Metric: Euclidean Distance\n",
    "\n",
    "Think of your data points as locations on a map. Euclidean distance is the straight-line path between them.\n",
    "\n",
    "For 2D points $(x_1, y_1)$ and $(x_2, y_2)$\n",
    "\n",
    "$$\n",
    "\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
    "$$\n",
    "\n",
    "In higher dimensions:  \n",
    "If A = $(a_1, a_2,..., a_n)$ and $B = (b_1, b_2,..., b_n)$, then:\n",
    "\n",
    "$$\n",
    "\\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\dots + (a_n - b_n)^2}\n",
    "$$\n",
    "\n",
    "Or more generally:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}\n",
    "$$\n",
    "\n",
    "Other examples are described in the book (e.g., Manhattan \"city block\" distance)\n",
    "\n",
    "\n",
    "## Problems with Euclidean Distance\n",
    "\n",
    "- Features with large values can dominate the distance.  \n",
    "  Example:  \n",
    "  `[Mileage = 18,457, Doors = 2, Age = 12]`  \n",
    "  → Mileage can overshadow the rest.\n",
    "\n",
    "### Solution: Normalize the Features\n",
    "\n",
    "Scale all features to the range [0, 1] using min-max normalization:\n",
    "\n",
    "```python\n",
    "(x - min(x)) / (max(x) - min(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### How to handle **categorical values** in distance calculations:\n",
    "\n",
    "- If two values are the same, their distance is 0 (no difference).  \n",
    "- For nominal categories (no natural order), if the values are different, distance is **1**.  \n",
    "  *Example:*  \n",
    "  If the attribute is \"color\" and one data point has \"red\" while another has \"blue\",  \n",
    "  then the distance = 1 because they are different. If both are \"red\", distance = 0.  \n",
    "- For **ordinal** categories (with order, like sizes: small < medium < large), you can:  \n",
    "  - Assign numeric values to categories and compute the numeric distance, or  \n",
    "  - Simply treat different values as distance 1 if you want to keep it simple.\n",
    "\n",
    "\n",
    "### What to do with **missing values**:\n",
    "\n",
    "- Ideally, clean your data first by removing or imputing missing values.  \n",
    "- If one value is missing and the other is known, be conservative and assume the **maximum distance**:  \n",
    "  - For nominal attributes, max distance = 1.  \n",
    "  - For ordinal attributes, max distance = 1 or the largest possible difference from the known value.  \n",
    "  - For continuous (numeric) attributes, instead of using maximum distance, it’s better to impute the missing value (e.g., using mean or median) before calculating distances to avoid distortion. \n",
    "- If both values are missing, treat their distance as the maximum (e.g., 1).\n",
    "\n",
    "> **Why?**  \n",
    "> Missing values can cause unreliable distance calculations, so assuming maximum difference prevents misleading closeness. For continuous values, imputation helps maintain reasonable distance calculations.\n",
    "\n",
    "\n",
    "### Distance metrics and feature weighting:\n",
    "\n",
    "- Most distance-based algorithms treat all features equally by default.  \n",
    "- Sometimes, some features are more important than others. Assigning weights lets you give these features more influence on the distance calculation.  \n",
    "- Removing irrelevant or noisy features called **feature reduction** can improve the model's accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: X_train, y_train, X_test, k\n",
    "row_distances = []\n",
    "y_predicted = []\n",
    "for test_instance in X_test:\n",
    "    for train_instance in X_train:\n",
    "        d = distance(train_instance, test_instance)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_instances = get_top_k_instances(row_distances, k) # TODO: need to write this function\n",
    "    prediction = select_class_label(top_k_instances)\n",
    "    y_predicted.append(prediction)\n",
    "```\n",
    "\n",
    "## kNN Example 1\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? \n",
    "\n",
    "Use kNN with $k$ = 3. Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "    \n",
    "### Some Notes\n",
    "Q: What happens if there are ties in the top-k distances (get_top_k)? E.g., which are top 3 in: [[.28,$r_1$],[.33,$r_2$],[.33,$r_3$],[.33,$r_4$],[.37,$r_5$]]?\n",
    "* Different options ... e.g.:\n",
    "    * Randomly select from ties\n",
    "    * Do top-k distances (instead of instances)\n",
    "    * Ignore ties (in case above, just use $r_1$ and $r_2$)\n",
    "\n",
    "Nearest doesn't imply near\n",
    "* top-k instances might not be that close to the instance being classified\n",
    "* Especially true as the number of attributes (\"dimensions\") increases\n",
    "    * An example of the \"curse of dimensionality\"\n",
    "* Again, have to use common sense and an understanding of the dataset\n",
    "\n",
    "## kNN Example 2\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have continuous values. Assume the data has been normalized by scaling to a value between 0 and 10.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "Assume we have the following instance to classify using $k$-NN for $k$ = 3 with \"majority voting\" to determine the class label from the k closest neighbors. \n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|2|3|?|\n",
    "\n",
    "1. What are the three closest neighbors of the following instance?\n",
    "1. How would you classify it?\n",
    "\n",
    "### Efficiency issues\n",
    "Q: Is k-NN efficient? Can you find any efficiency issues?\n",
    "* Given a training set with $D$ instances and $k = 1$\n",
    "* $O(D)$ comparisons needed to classify a given instance\n",
    "\n",
    "Q: Can you think of any ways to improve the efficiency?\n",
    "1. Use search trees\n",
    "    * Presort and arrange instances into a search tree\n",
    "    * Can reduce comparisons to $O(log D)$\n",
    "2. Check each training instance in parallel\n",
    "    * Gives $O(1)$ comparisons\n",
    "3. Editing/Pruning\n",
    "    * Filter or remove training tuples that prove useless\n",
    "    * Reduces size of $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Question\n",
    "1. In ClassificationFun/main.py, write a unit test for a function called `compute_euclidean_distance()` that accepts two feature vectors and returns the Euclidean distance between them: $dist(v1, v2) = \\sqrt{\\sum_{i=1}^{n}(v1_i - v2_i)^{2}}$\n",
    "    * Make up some values to call your function with\n",
    "    * Test your function's output against SciPy's `euclidean()` function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html\n",
    "    * Write the `compute_euclidean_distance(v1, v2)` function and test it with `pytest main.py`\n",
    "\n",
    "1. In ClassificationFun/main.py, consider the following labeled dataset, represented as the lists below. **Assume the two attributes have been normalized by scaling to a value between 0 and 10 (e.g., no additional normalization is necessary)**\n",
    "    1. Copy and paste the following lists into your main.py\n",
    "    1. Compute the distance between each train instance and the unseen instance: `[2, 3]`\n",
    "        * Print out the distances and inspect them...\n",
    "        * What are the k=3 closest neighbors of the following instance and how would you classify it?\n",
    "            * How can you programmatically answer this question?\n",
    "            * Hint: need a way to sort training instances by distance\n",
    "\n",
    "```python\n",
    "header = [\"att1\", \"att2\"]\n",
    "X_train = [\n",
    "    [3, 2],\n",
    "    [6, 6],\n",
    "    [4, 1],\n",
    "    [4, 4],\n",
    "    [1, 2],\n",
    "    [2, 0],\n",
    "    [0, 3],\n",
    "    [1, 6]\n",
    "]\n",
    "y_train = [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\"] # parallel to X_train\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Discussion\n",
    "1. Read through the `MySimpleLinearRegressionClassifier` docstrings in `myclassifiers.py`\n",
    "    * What is the purpose of `discretizer`?\n",
    "    * Functions in Python are objects, therefore a function name is a reference. See code snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'> <class 'function'> True\n",
      "10\n",
      "orig list: [3, 4, 5]\n",
      "map double to each elem: [6, 8, 10]\n",
      "filter each elem by odd: [3, 5]\n",
      "reduce elems to total value: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def double(x):\n",
    "    return 2 * x\n",
    "\n",
    "myfunc = double\n",
    "print(type(double), type(myfunc), double == myfunc)\n",
    "print(myfunc(5))\n",
    "\n",
    "# often used with map, filter, reduce\n",
    "nums = [3, 4, 5]\n",
    "print(\"orig list:\", nums)\n",
    "# map a function to each elem in a list\n",
    "print(\"map double to each elem:\", list(map(myfunc, nums)))\n",
    "\n",
    "# filter elems in a list by some criteria\n",
    "def odd(x):\n",
    "    return x % 2 == 1\n",
    "print(\"filter each elem by odd:\", list(filter(odd, nums)))\n",
    "\n",
    "# reduce elems in a list to a single value\n",
    "from functools import reduce\n",
    "def total(total_so_far, x):\n",
    "    return total_so_far + x\n",
    "print(\"reduce elems to total value:\", reduce(total, nums, 0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "03634b338fa45d156389a4bccc161271443b46ab00a525a3df16b6f48f1d4dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
