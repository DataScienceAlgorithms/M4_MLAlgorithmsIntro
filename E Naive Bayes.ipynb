{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Naive Bayes\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about Bayes Theorem\n",
    "* Learn about the Naive Bayes classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/22\n",
    "* Announcements\n",
    "    * LA7 posted and is due on end of Thursday\n",
    "    * Note on midterm grades: They are a combination of LAs, PAs, and IQs. I will be entering 0s for any missing LA1–6, PA1–2, and IQ1–4.\n",
    "* Naive Bayes Lab tasks #1 & #2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification\n",
    "Basic ideas\n",
    "* Predict class labels based on probabilities (statistics)\n",
    "* Naive Bayes comparable in performance to \"fancier\" approaches\n",
    "* Relatively efficient on large datasets\n",
    "* Assumes \"conditional independence\"\n",
    "    * Effect of one attribute on a class is independent from other attributes\n",
    "    * This is why it is called \"naive.\"\n",
    "    * Helps with execution time (speed)\n",
    "### Probability Basics\n",
    "* Conditional probabilities represent the probability of an event given some other event has occurred, which is represented with the following formula:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P(A \\cap B)$ = probability that both A and B occur  \n",
    "- $P(B)$ = probability that B occurs, P(B) is never equal to zero.\n",
    "  \n",
    "**Example:**  \n",
    "- Deck of 52 cards:  \n",
    "  - Let \\(A\\) = card is a King  \n",
    "  - Let \\(B\\) = card is a Spade  \n",
    "If you know the card is a Spade what is the probability it is a king?  \n",
    "$ P(B)=\\frac{13}{52}= \\frac{1}{4}$\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{1/52}{1/4} = \\frac{1}{13}\n",
    "$$\n",
    "\n",
    "\n",
    "## Bayes Theorem \n",
    "If $P(A \\cap B)$ is the probability that both $A$ and $B$ occur, then:\n",
    "$$P(A \\cap B) = P(A|B)P(B) = P(B|A)P(A)$$\n",
    "In other words:\n",
    "* Let's say $A$ occurs $x$% of the time given (within) $B$\n",
    "* And $B$ occurs $y$% of the time\n",
    "* Then $A$ and $B$ occur together, i.e., $A \\cap B$: $x$% $\\cdot y$% of the time\n",
    "\n",
    "\n",
    "For example:\n",
    "* Assume we have a bucket of Lego bricks\n",
    "* 50% of the 1x2 bricks are Red\n",
    "* 10% of the bricks in the bucket are 1x2's\n",
    "* Then, 50% of the 10% of 1x2's are Red-1x2's (i.e., 50% $\\cdot$ 10%)  \n",
    "\n",
    "This shows how we can compute the probability of two events happening together using conditional probability.\n",
    "Now, Bayes’ theorem simply rearranges this relationship to find the reverse conditional probability: \n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B|A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- The probability of \\(A\\) given \\(B\\) can be computed using the reverse conditional probability P(B|A) and the prior P(A).  \n",
    "\n",
    "### Bayes Theorem for Classification\n",
    "- Let:  \n",
    "  - H = hypothesis that an instance belongs to a class \\(C\\)  \n",
    "  - X = observed attribute values of the instance  \n",
    "\n",
    "Bayes Theorem becomes:\n",
    "$$\n",
    "P(H|X) = \\frac{P(X|H) P(H)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "$P(H)$ ... the probability of event $H$\n",
    "* $H$ (hypothesis) for us would be that any given instance is of a class $C$\n",
    "* Called the prior probability\n",
    "\n",
    "$P(X)$ ... the probability of event $X$\n",
    "* For us, $X$ would be an instance (a row in a table)\n",
    "* The probability that an instance would have $X$'s attribute values\n",
    "  \n",
    "$P(X|H)$ ... the conditional probability of $X$ given $H$\n",
    "* The probability of X’s attribute values assuming we know it is of class C\n",
    "\n",
    "$P(H|X)$ ... the conditional probability of $H$ given $X$\n",
    "* The probability that $X$ is of class $C$ given $X$'s attribute values\n",
    "* A posterior probability\n",
    "* This is the one we want to know to make predictions!\n",
    "    * i.e., we want the $C$ that gives the highest probability\n",
    "* We can estimate $P(H)$, $P(X)$, and $P(X|H)$ from the training set\n",
    "* From these, we can use Bayes Theorem to estimate $P(H|X)$: \n",
    "\n",
    "**Prediction:**  \n",
    "- Compute P(H|X) for all classes C.  \n",
    "- Assign the instance to the class with the **highest posterior probability**.  \n",
    "\n",
    "> Estimations of P(H), P(X|H), and P(X) are computed from the training dataset.\n",
    "  \n",
    "\n",
    "## Classification Approach\n",
    "Basic Approach:\n",
    "1. We're given an instance $X = [v_1, v_2, ..., v_n]$ to classify\n",
    "1. For each class $C_1, C_2, ... , C_m$, we want to find the class $C_i$ such that:\n",
    "$$P(C_i|X) > P(C_j|X) \\: \\textrm{for} \\: i \\leq j \\leq m, j \\neq i$$\n",
    "In other words, we want to find the class $C_i$ with the largest $P(C_i|X)$\n",
    "1. Use Bayes Theorem to find each $P(C|X)$, i.e., for each $C_i$ calculate:\n",
    "$$P(C_i|X) = P(X|C_i)P(C_i)$$\n",
    "We leave out $P(X)$ since it is the same for all classes ($C_i$'s)\n",
    "1. We estimate $P(C)$ as the percentage of $C$-labeled rows in the training set\n",
    "$$P(C) = \\frac{|C|}{D}$$\n",
    "where $|C|$ is the number of instances classified as $C$ in the training set and $D$ is the training set size\n",
    "\n",
    "1. We estimate $P(X|C_i)$ using the independence assumption of attributes:\n",
    "$$P(X|C_i) = \\prod_{k=1}^{n}P(v_k|C_i)$$\n",
    "\n",
    "Expanding this gives:\n",
    "$$\n",
    "P(X|C_i) = P(v_1|C_i) \\times P(v_2|C_i) \\times \\cdots \\times P(v_n|C_i)\n",
    "$$\n",
    "If attribute $k$ is categorical  \n",
    "* We estimate $P(v_k|C_i)$ as the percentage of instances with value $v_k$ (in attribute $k$) across training set instances of class $C_i$\n",
    "    \n",
    "Some notes:\n",
    "* Step 5 is an optimization: comparing entire rows is expensive (esp. if many attributes)\n",
    "* For smaller datasets, there may also not be any matches\n",
    "* Can extend the approach to support continuous attributes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Tasks\n",
    "### Lab Task 1\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have categorical values.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|1|5|yes|\n",
    "|2|6|yes|\n",
    "|1|5|no|\n",
    "|1|5|no|\n",
    "|1|6|yes|\n",
    "|2|6|no|\n",
    "|1|5|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Compute the priors for the dataset (e.g. what is $P(result = yes)$ and $P (result = no)$?)\n",
    "1. Compute the conditional probabilities for the dataset by making a table like Bramer 3.2 (e.g. what is $P(att1 = 1|result = yes)$, $P(att1 = 2|result = yes)$, $P(att2 = 5|result = yes)$, ...\n",
    "1. If $X = [1, 5]$, what is $P(result = yes|X)$ and $P(result = no|X)$ assuming conditional independence? Show your work.\n",
    "    1. What would the class label prediction be for the instance $X = [1, 5]$? Show your work.\n",
    "1. If $X = [1, 5]$, what is $P(result = yes|X)$ and $P(result = no|X)$ *without* assuming conditional independence? Show your work.\n",
    "    1. What would the class label prediction be for the instance $X = [1, 5]$? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 2\n",
    "Example adapted from [this Naive Bayes example](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "Suppose we have the following dataset that has four attributes and a class attribute (PLAY GOLF):\n",
    "\n",
    "|OUTLOOK\t|TEMPERATURE\t|HUMIDITY\t|WINDY\t|PLAY GOLF|\n",
    "|-|-|-|-|-|\n",
    "|Rainy\t|Hot\t|High\t|False\t|No|\n",
    "|Rainy\t|Hot\t|High\t|True\t|No|\n",
    "|Overcast\t|Hot\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|True\t|No|\n",
    "|Overcast\t|Cool\t|Normal\t|True\t|Yes|\n",
    "|Rainy\t|Mild\t|High\t|False\t|No|\n",
    "|Rainy\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|Normal\t|False\t|Yes|\n",
    "|Rainy\t|Mild\t|Normal\t|True\t|Yes|\n",
    "|Overcast\t|Mild\t|High\t|True\t|Yes|\n",
    "|Overcast\t|Hot\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|True\t|No|\n",
    "\n",
    "Suppose we have a new instance X = \\[Sunny, Hot, Normal, False\\]. \n",
    "1. What is $P(PLAY GOLF = Yes|X)$? \n",
    "1. What is $P(PLAY GOLF = No|X)$? \n",
    "1. What is the prediction for X?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
