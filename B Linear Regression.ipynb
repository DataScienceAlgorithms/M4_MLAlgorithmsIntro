{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Linear Regression\n",
    "What are our learning objectives for this lesson?\n",
    "* Calculate a least squares linear regression line\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes, Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "Imagine you’re a social media influencer (or trying to become one). You want to figure out how many likes you’ll get on a post based on how many followers you have. So, you gather data from your past posts and start noticing a trend -- more followers usually means more likes.\n",
    "\n",
    "That’s where linear regression comes in! It helps you draw a straight line through your data to predict likes for future posts.\n",
    "\n",
    "## Breaking It Down:\n",
    "\n",
    "* Input (independent variable): Number of followers -- because that’s what you control or know.\n",
    "\n",
    "* Output (dependent variable): Number of likes -- because it depends on how many followers you have.\n",
    "\n",
    "* The goal: learn the relationship between followers and likes so we can predict future post performance.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/main/figures/insta.png\" alt=\"Linear Regression\" width =\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Fit Line in Linear Regression\n",
    "In linear regression, the best-fit line is the straight line that nails the relationship between your input and output. It’s the line that keeps the gaps between the real data points and its predictions as tiny as possible.\n",
    "\n",
    "In simple linear regression, the goal is to find the best-fit line:\n",
    "\n",
    "$$\n",
    "\\hat{y} = m x + b\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\hat{y}$ is the predicted value (dependent variable)  \n",
    "- $x$ is the input (independent variable)  \n",
    "- $m$ is the slope of the line (how much $y$ changes when $x$ changes)  \n",
    "- $b$ is the intercept (the value of $y$ when $x = 0$)\n",
    "\n",
    "---\n",
    "### Least Squares Method\n",
    "\n",
    "To calculate the best-fit line, we use the **Least Squares Method**. This method finds the line that minimizes the sum of the squared differences between the actual values ($y$) and the predicted values ($\\hat{y}$). These squared differences are called **residuals**.  \n",
    "Each residual is the difference between an actual value and the predicted value:\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "The basic least squares approach:\n",
    "1. Calculate the mean $\\bar{x}$ of the $x$ values and the mean $\\bar{y}$ of the $y$ values\n",
    "    * note the line must go through the point ($\\bar{x}$, $\\bar{y}$)\n",
    "2. Calculate the slope using the means.\n",
    "3. Calculate the y intercept as b.\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "The Least Squares Method minimizes the sum of squared residuals, also known as the Sum of Squared Errors (SSE):\n",
    "\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "This is why it’s called the \"least squares\" method. We are finding the line that makes this total squared error as small as possible.\n",
    "\n",
    "In simple terms, the least squares method helps us find the values of $m$ and $b$ that make our predictions as close as possible to the real data points.\n",
    "\n",
    "\n",
    "### Formulas\n",
    "\n",
    "**Slope (m):**\n",
    "\n",
    "$$\n",
    "m = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "**Intercept (b):**\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m \\times \\bar{x}\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $x_i, y_i$ are data points  \n",
    "- $\\bar{x}, \\bar{y}$ are means of $x$ and $y$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Understanding Relationship in Data\n",
    "\n",
    "Before computing the best-fit line, we can assess how the variable relate to each other using metrics like covariance and correlation coefficient.\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "Covariance measures how two variables change together:\n",
    "\n",
    "$$\n",
    "\\text{cov} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Correlation Coefficient \\( r \\)\n",
    "\n",
    "The correlation coefficient \\( r \\) checks how strong and linear the relationship is:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "- Note: The bottom part essentially the same as the top just squared to strip away the signs .\n",
    "- \\( r = 1 \\): perfect positive linear correlation  \n",
    "- \\( r = -1 \\): perfect negative linear correlation  \n",
    "- \\( r = 0 \\): no linear relationship  \n",
    "You can also use this to calculate the correlation:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\text{cov}}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "You can also use this alternative formula for the **slope**:\n",
    "\n",
    "$$\n",
    "m = r \\cdot \\frac{\\sigma_y}{\\sigma_x}\n",
    "$$\n",
    "\n",
    "\n",
    "### Assesing Model Fit\n",
    "\n",
    "#### Standard Error of the Estimate\n",
    "\n",
    "The standard error measures the spread of the residuals (how far off predictions are):\n",
    "\n",
    "$$\n",
    "\\text{stderr} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\n",
    "$$\n",
    "\n",
    "- $(y_i - y^\\prime)$  is a residual\n",
    "- Standard error is like the standard deviation of residuals\n",
    "- Lower values indicate a better fit\n",
    "\n",
    "---\n",
    "\n",
    "#### Tips for Calculation\n",
    "\n",
    "- Use `numpy.std(x)` for standard deviation\n",
    "- Watch out for integer division: use `/`, not `//`\n",
    "- Use list comprehensions for compact calculations:\n",
    "\n",
    "```python\n",
    "sum([(x[i] - x_avg) * (y[i] - y_avg) for i in range(n)])\n",
    "```\n",
    "\n",
    "---\n",
    "Q: What does it mean if there is a strong (linear) correlation?\n",
    "\n",
    "A strong linear correlation between two variables means they change together in a consistent, predictable way. This can have several implications:\n",
    "\n",
    "- One variable might be unnecessary, since it can be inferred from the other — they carry similar information.\n",
    "- One variable can be used to predict the other, which is especially helpful if one is a target or class label.\n",
    "- This relationship makes linear regression a useful tool, as it models how one variable depends on another.\n",
    "\n",
    "In short: Strong linear correlation suggests predictability, possible redundancy, and supports using regression techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Simple Linear Regression Class\n",
    "\n",
    "Understand the concept of Simple Linear Regression and explore a Python implementation using a custom class.\n",
    "\n",
    "1. Create a folder and file\n",
    "   - Create a folder named `ClassificationFun` and create a `main.py` file inside it.\n",
    "   - We take some quick notes on programming interfaces for ML algorithms.\n",
    "\n",
    "2. Download the Linear Regressor module\n",
    "   - Download `my_simple_linear_regressor.py` from:\n",
    "     [https://github.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/ClassificationFun](https://github.com/DataScienceAlgorithms/M4_MLAlgorithmsIntro/ClassificationFun)\n",
    "   - Read through the module to understand how `fit()` and `predict()` are implemented.\n",
    "\n",
    "3. Create a MySimpleLinearRegressor object\n",
    "   - In `main.py`, create a `MySimpleLinearRegressor` object.\n",
    "   - Call `fit()` using our training data (`y = 2x + some noise`) and then `predict()` for an unseen instance.\n",
    "   - Example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from my_simple_linear_regressor import MySimpleLinearRegressor\n",
    "\n",
    "# Prepare training data\n",
    "X_train = [[val] for val in range(0, 100)]\n",
    "y_train = [row[0] * 2 + np.random.normal(0, 25) for row in X_train]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent \n",
    "\n",
    "Gradient descent is the backbone of the learning process for various algorithms, including linear regression, logistic regression, support vector machines, and neural networks which serves as a fundamental optimization technique to minimize the cost function of a model by iteratively adjusting the model parameters to reduce the difference between predicted and actual values, improving the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X_train = [[val] for val in range(0, 100)]\n",
    "y_train = [row[0] * 2 + np.random.normal(0, 25) for row in X_train]\n",
    "\n",
    "# Initialize parameters\n",
    "m = 0  # slope\n",
    "b = 0  # intercept\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "n = len(X_train)\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
